{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Exercise #06  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Exercise 6.1 (Decision Trees):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in subdirectory ``data`` and save it as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('train.csv', dtype=int) # read train data\n",
    "data_test = pd.read_csv('test.csv', dtype=int)   # read test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions ###\n",
    "Examine the structure of this data set using the head method,\n",
    "\n",
    "    data_train.head() ; data_test.head()\n",
    "    \n",
    "How many images are in the data set? What is the dimension of the feature vectors?\n",
    "What are the features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>87</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>126</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      0       0       0       0       0       0       0       0       9   \n",
       "1      1       0       0       0       0       0       0       0       0   \n",
       "2      2       0       0       0       0       0       0      14      53   \n",
       "3      2       0       0       0       0       0       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       8  ...       103        87        56         0         0         0   \n",
       "1       0  ...        34         0         0         0         0         0   \n",
       "2      99  ...         0         0         0         0        63        53   \n",
       "3       0  ...       137       126       140         0       133       224   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2        31         0         0         0  \n",
       "3       222        56         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Answers:__ There are 70,000 images the total dataset. The dimension of the feature vectors is 784. The features are the pixels that make up the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data and put it into arrays ``X_train``, ``y_train`` and ``X_test``, ``y_test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.drop('label', axis=1)\n",
    "y_train = data_train['label']\n",
    "X_test = data_test.drop('label', axis=1)\n",
    "y_test = data_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the $i^{th}$ image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label  9\n"
     ]
    }
   ],
   "source": [
    "i = 1                         #Put in a value for i\n",
    "plt.imshow(X_train.iloc[i,:].values.reshape([28,28])) \n",
    "plt.show()\n",
    "print('Label ',y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree ##\n",
    "Here is the basic call to learn a decision tree using the default values on the data ``[X_train,y_train]``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Tree_model = DecisionTreeClassifier().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.7978\n",
      "Tree depth = 48\n",
      "Number of Leaves =  4998\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy:', Tree_model.score(X_train, y_train))\n",
    "print('Test Accuracy:',Tree_model.score(X_test, y_test))\n",
    "print('Tree depth =', Tree_model.get_depth())\n",
    "print('Number of Leaves = ', Tree_model.get_n_leaves())\n",
    "#Tree_model.get_n_leaves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions ###\n",
    "\n",
    "__i.__ With the default values, what is the structure of the tree? For example, are there any limits on the number of leaves, is the depth of the tree constrained, and so on.\n",
    "> The tree is unconstrained. As max_leaf_nodes is not specified as a hyperparameter upon creating the classifier, there are no limits imposed on the number of leaves. Therefore, the maximum number of leaves are limitless. The minimum impurity decrease is 0.0 by default, therefore nodes split whenever there is any presence of impurity. Each of the classes have a weight of one on them. No pruning is performed on the classifier.\n",
    "\n",
    "__ii.__ How many leaves are in the tree? What is the purity of the leaves? What is the maximum depth of the tree?\n",
    "> There are 4998 leaves in the tree. Since the inimum impurity decrease is 0.0, all of the leaves are considered pure. The maximum depth of this tree is 48.\n",
    "\n",
    "__iii.__ What is the training accuracy of your model on the training data? Discuss your results and any conclusions that you may have. Were you surprised by the results?\n",
    "> The training accuracy is 100\\%. This is not a surprise as this decision tree is unconstrained. A training error of 0 will always be the case for an unconstrained tree.\n",
    "\n",
    "__iv.__ What is the accuracy of your decision tree on the test set? What does this tell you?\n",
    "> The test accuracy is 79.78\\%. This indicates that overfitting is taking place since the training error is 0% and the difference between the training and test accuracies are so large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: ###\n",
    "Find the confusion matrix for your classifier on the test set. What does it tell you? Are there any classes that the classifier is have more trouble with than others?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[734,   5,  20,  35,  10,   1, 175,   0,  18,   0],\n",
       "       [  7, 963,   4,  28,   5,   1,  10,   0,   2,   0],\n",
       "       [ 17,   5, 677,  17, 152,   1,  95,   1,  14,   0],\n",
       "       [ 45,  16,  17, 825,  49,   0,  37,   0,   9,   1],\n",
       "       [  6,   5, 147,  47, 678,   2, 109,   0,  16,   1],\n",
       "       [  3,   1,   3,   2,   1, 872,   1,  50,   7,  26],\n",
       "       [172,   3, 120,  37,  98,   4, 558,   0,  20,   2],\n",
       "       [  2,   0,   0,   0,   0,  64,   0, 867,   8,  65],\n",
       "       [ 13,   2,  11,   9,   7,  20,  14,   6, 903,   4],\n",
       "       [  1,   0,   1,   0,   0,  35,   1,  76,   3, 901]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#\n",
    "confusion_matrix(Tree_model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_High numbers: row4col2: Coats vs Pullovers, row6col0 Shirts vs T-Shirts/Tops,  row6col2 Shirts vs Pullovers, row0col6  T-Shirt/Top vs Shirts, row5col7: Sandals vs Sneakers_\n",
    "\n",
    "> __Answer:__ The confusion matrix is a table that describes the performance of a classifier on a set of test data where each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class. The higher the number in each row/column, the likelier that a predicted class will be confused for another. The classifier has trouble distinguishing between: Coats vs Pullovers, Shirts vs T-Shirts/Tops, Shirts vs Pullovers,  T-Shirt/Top, and Sandals vs Sneakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#\n",
    "all_accuracies = cross_val_score(estimator=Tree_model, X=X_train, y=y_train, cv=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79225    0.79125    0.79441667 0.79458333 0.79066667]\n",
      "Mean Accuracy     0.79\n"
     ]
    }
   ],
   "source": [
    "print(all_accuracies)\n",
    "print('Mean Accuracy %8.2f' %(all_accuracies.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "Decide how many folds that you would like to use, and use cross-validation to estimate the expected classification error of your decision tree. You probably want to use either five or ten folds. Explain the reason for your number of folds that you use. What is the advantage of choosing 10 folds versus 5? What do your results show? Is there any evidence that your tree is either underfitting or overfitting the data? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#\n",
    "all_accuracies = cross_val_score(estimator=Tree_model, X=X_train, y=y_train, cv=10) #Number of Folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.794      0.79466667 0.798      0.79583333 0.79666667 0.79983333\n",
      " 0.79683333 0.79433333 0.791      0.79133333]\n",
      "Mean Accuracy     0.80\n"
     ]
    }
   ],
   "source": [
    "print(all_accuracies)\n",
    "print('Mean Accuracy %8.2f' %(all_accuracies.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Answers:__\n",
    "I have chosen 10 folds as I believe that a more accurate measurement of classification error is more important than the larger run time it takes to perform it. The advantage of choosing 10 folds versus 5 is that there is less bias towards overestimating the true expected error. There is however, a drawback as a higher number of folds results in a higher variance and longer run time. My results show that despite the training error being 0\\%, there is a 20% accuracy in terms of cross validation. This means that the classifier is likely overfitting the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: ###\n",
    "Design a decision tree with min_samples_leaf=5 and max_depth=12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Tree_model_2 = DecisionTreeClassifier(min_samples_leaf=5, max_depth=12).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__i.__ What is the training error for your classifier.\n",
    "> The training error is about 12.4017%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8759833333333333\n",
      "Training Error: 0.12401666666666666\n",
      "Test Accuracy: 0.8199\n",
      "Tree depth = 12\n",
      "Number of Leaves =  906\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy:', Tree_model_2.score(X_train, y_train))\n",
    "print('Training Error:', 1-Tree_model_2.score(X_train, y_train))\n",
    "print('Test Accuracy:',Tree_model_2.score(X_test, y_test))\n",
    "print('Tree depth =', Tree_model_2.get_depth())\n",
    "print('Number of Leaves = ', Tree_model_2.get_n_leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ii.__ Estimate the expected error, compare your results to the tree that you designed in\n",
    "part (a), and discuss your results.\n",
    "> According to cross validation, the expected error is about 18%. This is superior to the classifier from part (a) in a number of ways. First, there is no evidence of overfitting the this time as the training accuracy is not 0 as in the previous classifier. Also, there is only a percentage difference of about 5% between the training accuracy and the expected accuracy of the new classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.819      0.81116667 0.82066667 0.816      0.81583333 0.81566667\n",
      " 0.82133333 0.82083333 0.81416667 0.80833333]\n",
      "Mean Accuracy     0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#\n",
    "all_accuracies = cross_val_score(estimator=Tree_model_2, X=X_train, y=y_train, cv=10) #Number of Folds = 10\n",
    "\n",
    "print(all_accuracies)\n",
    "print('Mean Accuracy %8.2f' %(all_accuracies.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__iii.__ Find the confusion matrix for your classifier. Discuss what you find. Are there any\n",
    "pairs of classes that seem to be confused more often than others? Which ones? Does\n",
    "it make sense?\n",
    ">There are still instances in which multiple pairs of classes are confused more than others these pairs are: T-Shirt/Top & Shirt, Pullover & Coat, Coat & Trouser, Shirt & Pullover, and Shirt & Coat. With the exception of maybe coat & trouser, it is understandable where these confusions come from. Although perhaps the classifier has trouble differentiating between trousers and coats that have a split towrds the bottom. Regardless of why, I would say that they make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[806,   9,  14,  54,   5,   1, 199,   0,  15,   2],\n",
       "       [  3, 958,   5,  13,   1,   1,   4,   0,   2,   0],\n",
       "       [ 19,   2, 723,  16, 108,   0, 111,   0,  16,   1],\n",
       "       [ 32,  19,  10, 843,  49,   1,  34,   0,   5,   1],\n",
       "       [ 12,   5, 153,  44, 765,   0, 114,   0,  15,   0],\n",
       "       [  3,   0,   2,   2,   1, 875,   2,  42,   8,  25],\n",
       "       [113,   5,  86,  26,  67,   3, 527,   0,  31,   3],\n",
       "       [  1,   0,   0,   0,   0,  60,   0, 909,   7,  71],\n",
       "       [ 11,   2,   6,   2,   4,  21,   9,   3, 900,   4],\n",
       "       [  0,   0,   1,   0,   0,  38,   0,  46,   1, 893]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#\n",
    "confusion_matrix(Tree_model_2.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search ###\n",
    "Here is an example of a grid search. You will need to modify the calls for the type of grid search that you would like to perfom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: ###\n",
    "Use a grid search to find the best parameters for a decision tree classifier on the data set\n",
    "X_train, y_train. Note: the numbers given in the example above were for illustration\n",
    "purposes only - you should determine what the best values to use would be. Determine\n",
    "which parameters to perform a grid search on along with the values to be evaluated.\n",
    "Report the best set of parameter values and the accuracy of the classifier. Note: If your\n",
    "grid search is taking too long, you may consider reducing the size of the data set.\n",
    "\n",
    "> See output below for reported values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 15, 'min_impurity_decrease': 0, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.8153833333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_param = { 'min_samples_leaf':[10, 20, 30, 40], 'max_depth':[5, 10, 15, 20], #I removed 20 after a previous run rendered it unnecesary \n",
    "              'criterion': ['gini', 'entropy'], 'min_samples_split':[2, 5, 10], 'min_impurity_decrease':[0, 0.5, 1.0]}\n",
    "gd_sr = GridSearchCV(estimator = Tree_model, param_grid=grid_param, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gd_sr.fit(X_train, y_train)\n",
    "best_parameters = gd_sr.best_params_\n",
    "print(best_parameters)\n",
    "best_result = gd_sr.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost ##\n",
    "A basic call to the AdaBoostClassifier class\n",
    "### Assignment: ###\n",
    "__(a)__ Use AdaBoostClassifier to design a strong classifier using the clothing data set created\n",
    "in part (2b). Use a decision stump as the base classifier, and set n_estimators=3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(n_estimators=3000)\n",
    "ada_fit = ada_clf.fit(X_train, y_train)\n",
    "#ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5322166666666667\n"
     ]
    }
   ],
   "source": [
    "ada_clf.score(X_train,y_train)\n",
    "print(ada_clf.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5305\n"
     ]
    }
   ],
   "source": [
    "print(ada_clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ Determine the accuracy of your model on the training data and estimate the expected\n",
    "error using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56216667 0.52475    0.5415     0.52066667 0.42066667]\n",
      "Mean Accuracy     0.51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#\n",
    "all_accuracies = cross_val_score(estimator=ada_clf, X=X_train, y=y_train, cv=5) #Number of Folds = 5\n",
    "\n",
    "print(all_accuracies)\n",
    "print('Mean Accuracy %8.2f' %(all_accuracies.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ Analyze and comment on your results. How do your results change if you were to use\n",
    "a decision tree with a maximum depth of two or four instead of one as you have with a\n",
    "decision stump classifier? Discuss your findings.\n",
    ">The Adaboost classifier was absolutely horrible. It's original accuracy was a laughable 51%. Changing the maximum depth to 2 only helps it slighttly by about 10%. All in all, this classifier is unsuitable for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf_2 = AdaBoostClassifier((DecisionTreeClassifier(max_depth = 2)), n_estimators = 3000)\n",
    "ada_fit_2 = ada_clf_2.fit(X_train, y_train)\n",
    "#ada_clf_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64535\n",
      "0.6368\n"
     ]
    }
   ],
   "source": [
    "ada_clf_2.score(X_train,y_train)\n",
    "print(ada_clf_2.score(X_train,y_train))\n",
    "print(ada_clf_2.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ How much is your design affected by the number of estimators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf_3 = AdaBoostClassifier(n_estimators=500)\n",
    "ada_fit_3 = ada_clf_3.fit(X_train, y_train)\n",
    "#ada_clf_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50765\n",
      "0.5043\n"
     ]
    }
   ],
   "source": [
    "ada_clf_3.score(X_train,y_train)\n",
    "print(ada_clf_3.score(X_train,y_train))\n",
    "print(ada_clf_3.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##\n",
    "Random forest example with out-of-bag estimate of ensemble classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: ###\n",
    "__(a)__ Design a random forest classifier for the data set of clothing articles. Use the out-of-bag samples to estimate the expected error in your random forest. How does this error compare to the training error?\n",
    ">There's only about a 3% difference between the training and oob errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=15, max_features='sqrt', min_samples_leaf=20,\n",
       "                       min_samples_split=10, n_estimators=200, oob_score=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(n_estimators=200, oob_score=True, max_depth =15, max_features = 'sqrt',\n",
    "                                     min_samples_leaf = 20, min_samples_split = 10)\n",
    "forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8988666666666667\n",
      "0.8684\n"
     ]
    }
   ],
   "source": [
    "forest_clf.score(X_train,y_train)\n",
    "print(forest_clf.score(X_train,y_train))\n",
    "print(forest_clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8661\n"
     ]
    }
   ],
   "source": [
    "print(forest_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ Investigate the effect of the number of trees and the number of features used in the design of each tree on the performance of your classifier. Describe/document what you find.\n",
    "\n",
    "> Changing the number of trees by decreasing the maximum depth causes the classifier to be less accurate. As I've chosen the the optimal depth via the Grid Search, it is likely that the accuracy will also decrease if I choose a depth above the one specified in part (a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, max_features='sqrt', min_samples_leaf=20,\n",
       "                       min_samples_split=10, n_estimators=200, oob_score=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(n_estimators=200, oob_score=True, max_depth =2, max_features = 'sqrt',\n",
    "                                     min_samples_leaf = 20, min_samples_split = 10)\n",
    "forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6067166666666667\n",
      "0.6013\n"
     ]
    }
   ],
   "source": [
    "forest_clf.score(X_train,y_train)\n",
    "print(forest_clf.score(X_train,y_train))\n",
    "print(forest_clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6011833333333333\n"
     ]
    }
   ],
   "source": [
    "print(forest_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ Compare your classifier to others that you have designed in terms of performance as well as computational complexity when performing classifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: ###\n",
    "Read the documentation on ExtraTreesClassifier to see what hyperparameters that you\n",
    "may define, and repeat parts (a) and (b) in the random forest exercise above. Note that if you\n",
    "want to use oob samples to estimate the expected error, it is necessary to set oob_score=True\n",
    "because the default is to use the whole dataset to build the tree. You will also want to detemine\n",
    "the appropriate number of estimators (trees) to use.\n",
    "\n",
    "__(a)__ Design a Extra Trees Classifier for the data set of clothing articles. Use the out-of-bag samples to estimate the expected error in your random forest. How does this error compare to the training error?\n",
    "> There is about a 3% difference between the oob error and the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=True, max_depth=15, max_features='sqrt',\n",
       "                     min_samples_leaf=20, n_estimators=200, oob_score=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#\n",
    "et_clf = ExtraTreesClassifier(n_estimators=200, oob_score=True, max_depth=15, min_samples_split=2,\n",
    "                              min_samples_leaf = 20, bootstrap = True, max_features = 'sqrt')\n",
    "et_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8676666666666667\n",
      "0.8506\n"
     ]
    }
   ],
   "source": [
    "et_clf.score(X_train,y_train)\n",
    "print(et_clf.score(X_train,y_train))\n",
    "print(et_clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8493166666666667\n"
     ]
    }
   ],
   "source": [
    "print(et_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ Investigate the effect of the number of trees and the number of features used in the design of each tree on the performance of your classifier. Describe/document what you find.\n",
    "\n",
    "> Once again, changing the number of trees by decreasing the maximum depth causes the classifier to be less accurate. As I've chosen the the optimal depth via the Grid Search, it is likely that the accuracy will also decrease if I choose a depth above the one specified in part (a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=True, max_depth=2, max_features='sqrt',\n",
       "                     min_samples_leaf=20, n_estimators=200, oob_score=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#\n",
    "et_clf_2 = ExtraTreesClassifier(n_estimators=200, oob_score=True, max_depth=2, min_samples_split=2,\n",
    "                              min_samples_leaf = 20, bootstrap = True, max_features = 'sqrt')\n",
    "et_clf_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59815\n",
      "0.5923\n"
     ]
    }
   ],
   "source": [
    "et_clf_2.score(X_train,y_train)\n",
    "print(et_clf_2.score(X_train,y_train))\n",
    "print(et_clf_2.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59565\n"
     ]
    }
   ],
   "source": [
    "print(et_clf_2.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting ##\n",
    "\n",
    "### Assignment: ###\n",
    "Use gradient boosting to design a classifier using your clothing data set. Describe/document\n",
    "what you find, and comment on how well your classifier peforms compared to the other classifiers that you have designed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=15, max_features='sqrt',\n",
       "                           min_samples_leaf=20, n_estimators=200,\n",
       "                           random_state=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=0, n_estimators = 200, max_depth=15, min_samples_split=2,\n",
    "                              min_samples_leaf = 20, max_features = 'sqrt')\n",
    "gb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9062\n"
     ]
    }
   ],
   "source": [
    "gb_clf.score(X_train,y_train)\n",
    "print(gb_clf.score(X_train,y_train))\n",
    "print(gb_clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[888,   2,  13,  18,   0,   0, 146,   0,   0,   0],\n",
       "       [  0, 983,   0,   6,   0,   0,   1,   0,   1,   0],\n",
       "       [ 10,   1, 837,  11,  49,   0,  74,   0,   8,   1],\n",
       "       [ 22,  13,  14, 935,  27,   0,  23,   0,   0,   0],\n",
       "       [  2,   0,  75,  13, 880,   0,  53,   0,   2,   0],\n",
       "       [  1,   0,   0,   0,   0, 956,   0,   8,   1,   3],\n",
       "       [ 70,   1,  54,  17,  42,   1, 693,   0,   5,   0],\n",
       "       [  0,   0,   0,   0,   0,  28,   0, 949,   2,  35],\n",
       "       [  7,   0,   7,   0,   2,   4,  10,   0, 981,   1],\n",
       "       [  0,   0,   0,   0,   0,  11,   0,  43,   0, 960]], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(gb_clf.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6000,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 6000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0, 6000,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0, 6000,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 6000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0, 6000,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0, 6000,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0, 6000,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 6000]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(gb_clf.predict(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#\n",
    "all_accuracies = cross_val_score(estimator=gb_clf, X=X_train, y=y_train, cv=2) #Number of Folds = 2\n",
    "\n",
    "print(all_accuracies_gb)\n",
    "print('Mean Accuracy %8.2f' %(all_accuracies_gb.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">At first glance, it looks like the Gradient Boosting Classifier looks like it's overfitting the data. However, a look at the confusion matrix shows that there is minimal confusion occurring between the classes. The only instance of major confusion is between T-Shirts and Shirts which is understandable from a visual standpoint. It's clear that the Gradient Boosting Classifier would be the best performing one thus far (if it wasn't for the 10% margin between the training error and the test error).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation ##\n",
    "\n",
    "### Assignment: ###\n",
    "Based on what you determine to be the best classifier and the best set of hyperparameters for that classifier, evaluate the error on the test set, X_test, y_test that you have not yet used in any decisions or design.\n",
    "\n",
    ">The best performing classifier is the _Random Forest Classifier_. There is only a difference of 3% between the training accuracy (89.8867%) and the oob score(86.61%). Not to mention that there is no overfitting. The best set of hyperparameters for that classifier max_depth=15, max_features='sqrt', min_samples_leaf=20, min_samples_split=10, n_estimators=200, oob_score=True.\n",
    "\n",
    ">__NOTE:__ If not for the overfitting, the best performing classifier might have been the Gradient Boosting Classifier. The accuracy on the test data was 0.9062, making it the most accurate classifier out of all of them. The best hyper-parameters as found by the grid search was  random_state=0, n_estimators = 200, max_depth=15, min_samples_split=2, min_samples_leaf = 20, and max_features = 'sqrt'. However, because overfitting truly is taking place (because of the previously mentioned 10% disparity in accuracies), the Gradient Boosting Classifier is immediately disqualified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
